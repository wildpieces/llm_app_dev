{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 트랜스포머 모델을 다루기 위한 허깅페이스 트랜스포머 라이브러리\n",
    "Huggingface팀이 개발한 트랜스포머 라이브러리는 공통된 인터페이스로 트랜스포머 모델을 활용할 수 있도록 지원함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 허깅페이스 트랜스포머 활용에 필요한 라이브러리 설치\n",
    "# pip install transformers==4.50.0 datasets==3.5.0 huggingface_hub==0.29.0 -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 허깅페이스 트랜스포머란\n",
    "다양한 트랜스포머 모델을 통일된 인터페이스로 사용할 수 있도록 지원하는 오픈소스 라이브러리 <br>\n",
    "- transformers 라이브러리: 트렌스포머 모델과 토크나이저를 활용할 때 사용\n",
    "- datasets 라이브러리: 데이터셋을 공개하고 쉽게 가져다 쓸 수 있도록 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3762f50c8edd4a9babf3e371d1714ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c48da7ef58405c8eb54b25d74c6ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206a296998774ab5b81a98b1f0ecb32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4347648d184798bb311d2e8c4cea08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827adf64de1948adb3cc5c5cd877c30b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.3009,  0.0158,  0.0698,  ..., -0.3406,  0.5976,  0.5820],\n",
      "         [-0.1109,  0.0754, -0.1906,  ...,  0.2970,  0.4278, -0.0391],\n",
      "         [-0.5813, -0.0042,  0.4034,  ..., -0.2549,  0.2216,  0.8121],\n",
      "         ...,\n",
      "         [ 0.9971,  0.3301, -0.0688,  ..., -0.4873,  0.0168, -0.0345],\n",
      "         [-0.2394, -0.0573, -0.5885,  ..., -0.0415,  0.3123, -0.0288],\n",
      "         [ 0.7884,  0.4039,  0.0217,  ...,  0.3869, -0.4785, -0.4116]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-8.7247e-01, -3.1464e-01, -5.2733e-01,  7.3685e-01,  1.8460e-01,\n",
      "         -1.8295e-01,  8.9985e-01,  2.9762e-01, -3.9314e-01, -9.9994e-01,\n",
      "          7.1272e-02,  7.2618e-01,  9.7232e-01,  3.2968e-01,  9.1369e-01,\n",
      "         -7.5043e-01, -3.5754e-01, -5.6530e-01,  2.8968e-01, -6.4040e-01,\n",
      "          6.0199e-01,  9.9856e-01,  3.8675e-01,  2.9078e-01,  4.0431e-01,\n",
      "          7.7010e-01, -7.6515e-01,  9.1830e-01,  9.4834e-01,  7.2557e-01,\n",
      "         -7.3735e-01,  1.9867e-01, -9.8172e-01, -1.8334e-01, -4.9430e-01,\n",
      "         -9.8059e-01,  2.6051e-01, -7.4618e-01,  4.8118e-02,  5.9711e-02,\n",
      "         -8.1904e-01,  2.1572e-01,  9.9974e-01, -6.2062e-01,  1.3180e-02,\n",
      "         -3.7181e-01, -1.0000e+00,  2.5721e-01, -8.5921e-01,  3.5142e-01,\n",
      "          4.2221e-01,  2.7743e-02,  1.5075e-01,  3.6331e-01,  4.4038e-01,\n",
      "          2.4008e-02, -1.7561e-01,  6.5655e-02, -2.2944e-01, -5.1775e-01,\n",
      "         -6.1054e-01,  4.0539e-01, -4.9569e-01, -8.7416e-01,  5.6503e-01,\n",
      "          5.4685e-01, -9.5151e-02, -2.8588e-01, -6.3585e-02, -5.9148e-02,\n",
      "          8.8904e-01,  1.0488e-01,  2.8848e-01, -8.3510e-01,  1.3450e-01,\n",
      "          2.2096e-01, -6.5445e-01,  1.0000e+00, -5.5778e-01, -9.5749e-01,\n",
      "          4.4432e-01,  5.2114e-01,  5.8760e-01,  2.3121e-02,  3.6001e-01,\n",
      "         -1.0000e+00,  2.6260e-01, -6.2052e-02, -9.8500e-01,  2.5122e-01,\n",
      "          4.3635e-01, -1.6588e-01,  3.9133e-01,  5.3988e-01, -5.7578e-01,\n",
      "         -1.7328e-01, -8.3042e-02, -3.6534e-01, -1.3268e-01, -1.2891e-01,\n",
      "          2.1034e-02, -2.0756e-01, -2.4547e-01, -3.3774e-01,  2.4423e-01,\n",
      "         -4.7804e-01, -4.5389e-01,  3.9510e-01,  7.4779e-02,  6.1011e-01,\n",
      "          5.0728e-01, -3.0130e-01,  3.7723e-01, -9.3758e-01,  4.9906e-01,\n",
      "         -3.2958e-01, -9.8185e-01, -6.1150e-01, -9.7898e-01,  6.7381e-01,\n",
      "         -6.0385e-02, -2.6210e-01,  9.3328e-01,  1.9747e-01,  3.5069e-01,\n",
      "         -7.0665e-02, -3.4704e-01, -1.0000e+00, -2.7822e-01, -3.7445e-01,\n",
      "         -1.7645e-01, -2.1203e-01, -9.6432e-01, -9.4622e-01,  5.7054e-01,\n",
      "          9.4001e-01,  1.8941e-01,  9.9923e-01, -1.5256e-01,  9.1510e-01,\n",
      "          7.8499e-02, -5.6503e-01, -7.2291e-02, -4.5555e-01,  5.5808e-01,\n",
      "          1.8401e-01, -6.0199e-01,  2.3481e-01, -3.3688e-02, -2.7570e-01,\n",
      "         -5.3239e-01, -1.7641e-01, -4.0169e-01, -9.2079e-01, -3.1998e-01,\n",
      "          9.3809e-01, -7.1152e-02, -6.9783e-01,  6.7254e-02, -2.0274e-01,\n",
      "         -3.7808e-01,  8.4576e-01,  5.0007e-01,  3.6423e-01, -4.1740e-01,\n",
      "          3.6109e-01,  1.4876e-01,  3.5860e-01, -8.4838e-01,  2.6076e-01,\n",
      "          4.1682e-01, -3.1267e-01, -5.2371e-01, -9.6102e-01, -2.9391e-01,\n",
      "          5.4447e-01,  9.8303e-01,  7.4894e-01,  2.8395e-01,  1.4951e-01,\n",
      "         -2.6745e-01,  1.8009e-01, -9.3940e-01,  9.6656e-01, -1.0958e-01,\n",
      "          1.8235e-01,  1.6497e-01,  2.4865e-01, -8.2815e-01,  7.0421e-04,\n",
      "          8.1064e-01, -1.2264e-02, -8.1630e-01, -4.3830e-02, -4.7633e-01,\n",
      "         -3.8495e-01, -3.8118e-01,  4.5080e-01, -1.9782e-01, -3.4960e-01,\n",
      "          1.1252e-01,  8.9026e-01,  9.7578e-01,  6.9775e-01, -1.2651e-01,\n",
      "          5.7326e-01, -8.3355e-01, -4.0410e-01,  5.5685e-02,  1.7855e-01,\n",
      "          8.9321e-02,  9.8696e-01, -3.9308e-01, -7.7631e-02, -9.1122e-01,\n",
      "         -9.7244e-01,  5.5038e-03, -8.4574e-01, -2.4717e-02, -6.3462e-01,\n",
      "          4.9246e-01,  5.5000e-01,  1.1419e-01,  3.0862e-01, -9.6484e-01,\n",
      "         -7.7138e-01,  3.5477e-01, -2.1973e-01,  4.2417e-01, -2.4453e-01,\n",
      "          6.6965e-01,  6.5434e-01, -6.3527e-01,  8.0241e-01,  9.1417e-01,\n",
      "         -4.4787e-01, -6.7925e-01,  8.3256e-01, -2.6945e-01,  8.7207e-01,\n",
      "         -5.2281e-01,  9.8420e-01,  4.2696e-01,  4.8134e-01, -8.7702e-01,\n",
      "         -5.7352e-01, -8.7672e-01, -3.6707e-01,  7.0802e-02, -1.4201e-01,\n",
      "          6.3290e-01,  6.3346e-01,  2.9797e-01,  4.1055e-01, -6.0411e-01,\n",
      "          9.9386e-01, -7.6779e-01, -9.4760e-01,  2.8060e-01,  1.7030e-02,\n",
      "         -9.8417e-01,  3.4940e-01,  2.5224e-01, -4.0932e-01, -3.8472e-01,\n",
      "         -6.1061e-01, -9.4425e-01,  8.4996e-01,  1.8030e-01,  9.8473e-01,\n",
      "         -8.3624e-02, -8.9589e-01, -4.1394e-01, -8.9448e-01, -2.8790e-01,\n",
      "         -1.7622e-01,  1.2934e-01, -1.4377e-01, -9.4701e-01,  4.8915e-01,\n",
      "          5.5083e-01,  3.5019e-01, -5.7310e-01,  9.9580e-01,  9.9998e-01,\n",
      "          9.5247e-01,  8.9147e-01,  8.6635e-01, -9.9394e-01, -3.3778e-01,\n",
      "          9.9998e-01, -9.1078e-01, -1.0000e+00, -9.3294e-01, -5.2658e-01,\n",
      "          3.7828e-01, -1.0000e+00, -3.1776e-01,  7.3381e-02, -8.6885e-01,\n",
      "          3.6298e-01,  9.6532e-01,  9.8247e-01, -1.0000e+00,  8.0954e-01,\n",
      "          9.0122e-01, -6.6414e-01,  6.7350e-01, -3.1281e-01,  9.6308e-01,\n",
      "          5.0662e-01,  4.2693e-01, -2.2366e-01,  3.7596e-01, -8.1644e-01,\n",
      "         -8.3308e-01, -1.3696e-01, -4.6714e-01,  9.4581e-01,  1.8325e-01,\n",
      "         -7.8852e-01, -8.7354e-01, -5.8153e-02, -6.9012e-02, -3.4173e-01,\n",
      "         -9.4760e-01, -1.7820e-01, -1.3929e-01,  6.8355e-01,  4.5168e-02,\n",
      "          2.6814e-01, -6.8768e-01,  2.1210e-01, -2.3396e-01,  3.4735e-01,\n",
      "          6.8611e-01, -9.2963e-01, -5.2597e-01,  3.0077e-01, -3.7891e-01,\n",
      "         -4.7398e-01, -9.5464e-01,  9.4878e-01, -3.2371e-01,  3.0548e-01,\n",
      "          1.0000e+00, -1.6540e-01, -7.9525e-01,  5.4072e-01,  1.6751e-01,\n",
      "         -1.2475e-01,  1.0000e+00,  6.4986e-01, -9.5677e-01, -6.0413e-01,\n",
      "          3.7020e-01, -4.8581e-01, -4.7777e-01,  9.9866e-01, -2.5729e-01,\n",
      "         -2.4091e-01,  1.1121e-01,  9.6287e-01, -9.8189e-01,  9.3767e-01,\n",
      "         -8.7587e-01, -9.4305e-01,  9.3710e-01,  9.0091e-01, -3.8187e-01,\n",
      "         -6.1976e-01,  1.4977e-01, -5.8692e-01,  2.6118e-01, -9.5406e-01,\n",
      "          6.2279e-01,  4.8689e-01, -8.8539e-02,  8.5752e-01, -8.3492e-01,\n",
      "         -5.3720e-01,  2.5680e-01, -4.1266e-01,  2.2846e-01,  5.8122e-01,\n",
      "          4.3429e-01, -2.4578e-01,  1.2280e-01, -2.9919e-01, -3.1362e-01,\n",
      "         -9.6913e-01,  6.2339e-02,  1.0000e+00,  1.3628e-02, -1.2817e-01,\n",
      "         -3.8745e-01,  1.8794e-02, -1.8447e-01,  4.2568e-01,  5.1443e-01,\n",
      "         -2.2532e-01, -8.0441e-01,  1.4079e-01, -9.4743e-01, -9.8253e-01,\n",
      "          7.6432e-01,  1.4451e-01, -3.2291e-01,  9.9988e-01,  3.1823e-01,\n",
      "          1.5380e-01,  1.3672e-01,  7.6152e-01,  1.0233e-01,  5.8834e-01,\n",
      "          5.5959e-01,  9.6941e-01, -2.7468e-01,  6.1814e-01,  8.2141e-01,\n",
      "         -5.6555e-01, -2.7680e-01, -6.2172e-01, -8.9433e-02, -9.0074e-01,\n",
      "          1.2009e-01, -9.2751e-01,  9.4329e-01,  4.9252e-01,  2.7484e-01,\n",
      "          2.1843e-01,  6.6066e-02,  1.0000e+00, -2.2593e-01,  6.4518e-01,\n",
      "         -5.1134e-01,  8.2547e-01, -9.8957e-01, -7.8994e-01, -3.4982e-01,\n",
      "         -8.3618e-02, -3.6136e-01, -2.2800e-01,  2.0661e-01, -9.5846e-01,\n",
      "          3.5442e-01,  9.7109e-02, -9.7417e-01, -9.8264e-01,  2.9154e-01,\n",
      "          8.3737e-01,  6.6634e-02, -8.0501e-01, -6.1920e-01, -5.9091e-01,\n",
      "          1.7083e-01, -1.7838e-01, -9.0619e-01,  2.4375e-01, -1.7610e-01,\n",
      "          4.1765e-01, -2.2214e-01,  5.1794e-01,  5.8822e-01,  6.4995e-01,\n",
      "          1.3124e-01, -9.7189e-02, -8.3228e-02, -8.4713e-01,  8.4501e-01,\n",
      "         -7.9905e-01, -4.0447e-01, -2.0988e-01,  1.0000e+00, -5.1438e-01,\n",
      "          6.3233e-01,  7.9540e-01,  6.0073e-01, -1.3826e-01,  2.1753e-01,\n",
      "          6.5091e-01,  1.4618e-01, -6.2026e-01, -4.0101e-01, -7.0176e-01,\n",
      "         -3.3691e-01,  6.5601e-01, -8.1212e-02,  4.6828e-01,  6.5125e-01,\n",
      "          4.2083e-01,  1.4356e-01, -3.9187e-02, -4.4730e-02,  9.9797e-01,\n",
      "         -1.2616e-01,  5.6322e-02, -5.0923e-01,  7.9205e-03, -3.3530e-01,\n",
      "         -4.9021e-01,  1.0000e+00,  2.7262e-01, -1.3420e-01, -9.8275e-01,\n",
      "         -3.7159e-01, -8.9745e-01,  9.9994e-01,  8.0115e-01, -7.2103e-01,\n",
      "          6.0988e-01,  2.2475e-01, -1.7136e-01,  7.3667e-01, -1.3005e-01,\n",
      "         -2.5390e-01,  2.7347e-01,  9.6592e-02,  9.2964e-01, -4.7674e-01,\n",
      "         -9.5305e-01, -6.8485e-01,  3.5561e-01, -9.3918e-01,  9.9616e-01,\n",
      "         -4.4200e-01, -1.8582e-01, -4.1399e-01,  4.5030e-01,  6.3904e-01,\n",
      "         -1.7178e-01, -9.7449e-01, -5.1296e-02,  1.6738e-01,  9.3006e-01,\n",
      "          1.3585e-01, -5.9081e-01, -9.1515e-01,  1.1425e-01,  4.2950e-01,\n",
      "         -5.2346e-01, -8.7475e-01,  9.4331e-01, -9.7478e-01,  3.9603e-01,\n",
      "          1.0000e+00,  3.9404e-01, -6.0146e-01,  2.9373e-02, -5.1601e-01,\n",
      "          2.4861e-01,  2.4923e-01,  6.7270e-01, -9.2070e-01, -2.8287e-01,\n",
      "         -7.6749e-02,  1.7067e-01, -8.0916e-02,  1.2053e-01,  5.9641e-01,\n",
      "          1.7944e-01, -5.4069e-01, -5.6923e-01, -2.5089e-02,  4.6722e-01,\n",
      "          8.3220e-01, -3.5286e-01, -1.6219e-01,  9.4050e-02, -9.4970e-02,\n",
      "         -8.7677e-01, -2.7396e-01, -2.1794e-01, -9.9938e-01,  7.5556e-01,\n",
      "         -1.0000e+00, -1.7111e-01, -1.8512e-01, -2.3656e-01,  7.9198e-01,\n",
      "          2.8102e-01,  2.7825e-01, -6.7673e-01, -5.1072e-01,  6.6396e-01,\n",
      "          6.7959e-01, -1.5748e-01,  1.9009e-01, -6.7916e-01,  1.1148e-01,\n",
      "         -4.1106e-03, -7.4858e-03, -1.5137e-01,  8.2224e-01, -1.6503e-01,\n",
      "          1.0000e+00,  1.4966e-01, -6.3775e-01, -9.6420e-01,  2.2951e-01,\n",
      "         -2.5219e-01,  1.0000e+00, -9.1262e-01, -9.3021e-01,  2.5156e-01,\n",
      "         -6.5498e-01, -8.3972e-01,  2.0794e-01,  1.4597e-01, -5.7221e-01,\n",
      "         -5.5899e-01,  9.3297e-01,  9.1276e-01, -6.3061e-01,  3.3351e-01,\n",
      "         -3.3155e-01, -3.4310e-01,  5.5119e-02,  1.7480e-01,  9.7497e-01,\n",
      "          1.8068e-01,  8.6350e-01,  4.1295e-01,  6.8414e-02,  9.4591e-01,\n",
      "          2.3672e-01,  4.7458e-01,  5.9380e-02,  1.0000e+00,  2.6528e-01,\n",
      "         -8.8648e-01,  3.4260e-01, -9.7573e-01, -1.5098e-01, -9.4319e-01,\n",
      "          2.2633e-01,  2.1388e-01,  8.7123e-01, -9.6188e-02,  9.3925e-01,\n",
      "          2.4525e-02, -9.8376e-02, -1.0091e-01,  1.4681e-01,  3.8430e-01,\n",
      "         -8.7014e-01, -9.7812e-01, -9.7528e-01,  3.6960e-01, -3.8164e-01,\n",
      "         -4.7706e-02,  1.4460e-01,  1.0588e-01,  3.0843e-01,  4.1187e-01,\n",
      "         -1.0000e+00,  9.1188e-01,  3.6028e-01,  7.0124e-01,  9.3118e-01,\n",
      "          5.1771e-01,  2.7587e-01,  2.8873e-01, -9.7848e-01, -9.4348e-01,\n",
      "         -3.1992e-01, -2.0700e-01,  6.7060e-01,  6.4204e-01,  8.7050e-01,\n",
      "          3.0572e-01, -4.9177e-01, -2.6046e-01,  1.0768e-01, -5.5766e-01,\n",
      "         -9.8769e-01,  3.4500e-01, -1.2493e-01, -9.4611e-01,  9.4435e-01,\n",
      "         -2.2333e-01, -1.4135e-01,  1.9776e-01, -3.9046e-01,  9.3880e-01,\n",
      "          6.8928e-01,  3.8767e-01,  1.1025e-01,  5.1922e-01,  8.3273e-01,\n",
      "          9.4730e-01,  9.7341e-01, -5.7427e-01,  7.8560e-01, -6.8621e-02,\n",
      "          4.1192e-01,  6.9466e-01, -9.3774e-01,  2.3971e-01,  1.3474e-01,\n",
      "         -4.2771e-01,  1.6046e-01, -2.4939e-01, -9.7068e-01,  5.0689e-01,\n",
      "         -2.9860e-01,  5.1771e-01, -3.1591e-01,  1.0868e-01, -3.8842e-01,\n",
      "         -1.5632e-01, -6.4045e-01, -5.4585e-01,  6.6964e-01,  3.3962e-01,\n",
      "          8.5658e-01,  5.4788e-01, -1.0526e-01, -5.1888e-01, -1.6210e-01,\n",
      "         -5.4360e-01, -8.8313e-01,  8.7812e-01,  4.3411e-02, -2.1279e-01,\n",
      "          3.2289e-01, -1.8518e-01,  6.5461e-01, -6.4463e-02, -3.7191e-01,\n",
      "         -3.7232e-01, -7.5502e-01,  8.0540e-01, -1.7699e-01, -5.2501e-01,\n",
      "         -5.6918e-01,  3.7711e-01,  3.8391e-01,  9.9864e-01, -5.3484e-01,\n",
      "         -5.4315e-01, -7.2384e-02, -2.4831e-01,  4.2074e-01, -2.2461e-01,\n",
      "         -1.0000e+00,  2.4747e-01,  7.5572e-02,  3.7574e-01, -7.1563e-02,\n",
      "          3.3389e-01, -1.6045e-01, -9.6561e-01, -1.6314e-01,  7.3813e-02,\n",
      "          3.5186e-01, -4.4838e-01, -2.2300e-01,  5.7876e-01,  6.1218e-01,\n",
      "          7.0053e-01,  8.2005e-01,  1.1350e-01,  2.0501e-01,  6.4786e-01,\n",
      "         -2.2238e-01, -6.3388e-01,  8.9791e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# BERT와 GPT-2모델을 활용할 때 허깅페이스 트랜스포머 코드 비교\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "text = \"What is Huggingface Transformers?\"\n",
    "# BERT 모델 활용\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\") # 모델 불러오기\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') # 토크나이저 불러오기\n",
    "encoded_input = bert_tokenizer(text, return_tensors='pt') # 입력 토큰화\n",
    "bert_output = bert_model(**encoded_input) # 모델에 입력\n",
    "print(bert_output)\n",
    "# # GPT-2 모델 활용\n",
    "# gpt_model = AutoModel.from_pretrained('gpt2') # 모델 불러오기\n",
    "# gtp_tokenizer = AutoTokenizer.from_pretrained('gpt2') # 토크나이저 불러오기\n",
    "# encoded_input = gpt_tokenizer(text, return_tensors='pt') # 입력 토큰화\n",
    "# gpt_output = gpt_model(**encoded_input) # 모델에 입력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 허깅페이스 허브 탐색하기\n",
    "### 3.2.1 모델 허브\n",
    "어떤 작업(Tasks)에 사용하는지, 어떤 언어(Languages)로 학습된 모델인지 등 자양한 기준으로 모델이 분류됨 <br>\n",
    "모델 허브에서는 자연어 처리(NLP)뿐만 아니라 컴퓨터 비전, 오디오 처리, 멀티 모달 등 다양한 작업 분야의 모델을 제공 <br>\n",
    "### 3.2.2 데이터셋 허브\n",
    "분류 기준에 데이터셋 크기, 데이터 유형 등이 추가로 있고 선택한 기준에 맞는 데이터셋을 보여줌 <br>\n",
    "### 3.2.3 모델 데모를 공개하고 사용할 수 있는 스페이스\n",
    "사용자가 자신의 모델 데모를 간편하게 공개 <br>\n",
    "스페이스를 활용하면 별도의 복잡한 웹 페이지 개발 없이 모델 데모를 공유할 수 있음 <br>\n",
    "## 3.3 허깅페이스 라이브러리 사용법 익히기\n",
    "모델을 학습시키거나 추론하기 위해서는 모델, 토크나이저, 데이터셋이 필요\n",
    "### 3.3.1 모델 활용하기\n",
    "허깅페이스에서는 모델을 바디(body)와 헤드(head)로 구분함.  같은 바디를 사용하면서 다른 작업에 사용할 수 있도록 만들기 위해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deaccfc4547d47cd8c305f5ccf7245f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/546 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--klue--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60e436d9b0346af8fdd2b27497ceccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(32000, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "model_id = 'klue/roberta-base'\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AutoModel: 모델의 바디를 불러오는 클래스\n",
    "    - from_pretrained()메서드에서 인자로 받는 model_id 에 맞춰 적절한 클래스를 가져옴\n",
    "- model_id\n",
    "    - 허깅페이스 모델 허브의 저장소 경로(klue/roberta-base)인 경우 모델 허브에서 모델을 다운로드\n",
    "    - 로컬인 경우, 지정한 로컬 경로에서 모델을 불러옴\n",
    "<br>\n",
    "\n",
    "_AutoModel은 어떻게 klue/roberta-base 저장소의 모델이 RoBERTa계열의 모델인지 알 수 있을까?_\n",
    "- 허깅페이스 모델을 저장할 때 config.json이 함께 저장됨\n",
    "- 해당 설정 파일에 모델의 종류(model_type)\n",
    "- 여러 설정 파라미터(num_attention_heads, num_hidden_layers 등), 어휘 사전 크기(vocab_size), 토크나이저 클래스(tokenzier_class)등이 저장됨\n",
    "- AutoModel 과 AutoTokenizer 클래스는 config.json 파일을 참고해 적절한 모델과 토크나이저를 불러옴옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28f636e2bd540dc9696e219229f6925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--SamLowe--roberta-base-go_emotions. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cbbd3bb149e4e1f8e325a867bf6c9ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=28, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 분류 헤드가 포함된 모델 불러오기\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model_id = 'SamLowe/roberta-base-go_emotions'\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "classification_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (yy)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
