{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 트랜스포머 모델을 다루기 위한 허깅페이스 트랜스포머 라이브러리\n",
    "Huggingface팀이 개발한 트랜스포머 라이브러리는 공통된 인터페이스로 트랜스포머 모델을 활용할 수 있도록 지원함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 허깅페이스 트랜스포머 활용에 필요한 라이브러리 설치\n",
    "# pip install transformers==4.50.0 datasets==3.5.0 huggingface_hub==0.29.0 -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 허깅페이스 트랜스포머란\n",
    "다양한 트랜스포머 모델을 통일된 인터페이스로 사용할 수 있도록 지원하는 오픈소스 라이브러리 <br>\n",
    "- transformers 라이브러리: 트렌스포머 모델과 토크나이저를 활용할 때 사용\n",
    "- datasets 라이브러리: 데이터셋을 공개하고 쉽게 가져다 쓸 수 있도록 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.3009,  0.0158,  0.0698,  ..., -0.3406,  0.5976,  0.5820],\n",
      "         [-0.1109,  0.0754, -0.1906,  ...,  0.2970,  0.4278, -0.0391],\n",
      "         [-0.5813, -0.0042,  0.4034,  ..., -0.2549,  0.2216,  0.8121],\n",
      "         ...,\n",
      "         [ 0.9971,  0.3301, -0.0688,  ..., -0.4873,  0.0168, -0.0345],\n",
      "         [-0.2394, -0.0573, -0.5885,  ..., -0.0415,  0.3123, -0.0288],\n",
      "         [ 0.7884,  0.4039,  0.0217,  ...,  0.3869, -0.4785, -0.4116]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-8.7247e-01, -3.1464e-01, -5.2733e-01,  7.3685e-01,  1.8460e-01,\n",
      "         -1.8295e-01,  8.9985e-01,  2.9762e-01, -3.9314e-01, -9.9994e-01,\n",
      "          7.1272e-02,  7.2618e-01,  9.7232e-01,  3.2968e-01,  9.1370e-01,\n",
      "         -7.5043e-01, -3.5754e-01, -5.6530e-01,  2.8968e-01, -6.4040e-01,\n",
      "          6.0199e-01,  9.9856e-01,  3.8675e-01,  2.9078e-01,  4.0431e-01,\n",
      "          7.7010e-01, -7.6515e-01,  9.1830e-01,  9.4834e-01,  7.2557e-01,\n",
      "         -7.3735e-01,  1.9867e-01, -9.8172e-01, -1.8334e-01, -4.9430e-01,\n",
      "         -9.8059e-01,  2.6051e-01, -7.4618e-01,  4.8118e-02,  5.9711e-02,\n",
      "         -8.1904e-01,  2.1572e-01,  9.9974e-01, -6.2062e-01,  1.3180e-02,\n",
      "         -3.7181e-01, -1.0000e+00,  2.5721e-01, -8.5921e-01,  3.5142e-01,\n",
      "          4.2221e-01,  2.7744e-02,  1.5075e-01,  3.6331e-01,  4.4038e-01,\n",
      "          2.4008e-02, -1.7561e-01,  6.5655e-02, -2.2944e-01, -5.1775e-01,\n",
      "         -6.1054e-01,  4.0539e-01, -4.9569e-01, -8.7416e-01,  5.6503e-01,\n",
      "          5.4685e-01, -9.5151e-02, -2.8588e-01, -6.3586e-02, -5.9148e-02,\n",
      "          8.8904e-01,  1.0488e-01,  2.8848e-01, -8.3510e-01,  1.3450e-01,\n",
      "          2.2096e-01, -6.5445e-01,  1.0000e+00, -5.5778e-01, -9.5749e-01,\n",
      "          4.4432e-01,  5.2114e-01,  5.8760e-01,  2.3121e-02,  3.6001e-01,\n",
      "         -1.0000e+00,  2.6260e-01, -6.2052e-02, -9.8500e-01,  2.5122e-01,\n",
      "          4.3635e-01, -1.6588e-01,  3.9133e-01,  5.3988e-01, -5.7578e-01,\n",
      "         -1.7328e-01, -8.3042e-02, -3.6534e-01, -1.3268e-01, -1.2891e-01,\n",
      "          2.1035e-02, -2.0756e-01, -2.4547e-01, -3.3774e-01,  2.4423e-01,\n",
      "         -4.7804e-01, -4.5389e-01,  3.9510e-01,  7.4780e-02,  6.1011e-01,\n",
      "          5.0728e-01, -3.0130e-01,  3.7723e-01, -9.3758e-01,  4.9906e-01,\n",
      "         -3.2958e-01, -9.8185e-01, -6.1150e-01, -9.7898e-01,  6.7381e-01,\n",
      "         -6.0385e-02, -2.6210e-01,  9.3328e-01,  1.9747e-01,  3.5069e-01,\n",
      "         -7.0666e-02, -3.4704e-01, -1.0000e+00, -2.7822e-01, -3.7445e-01,\n",
      "         -1.7645e-01, -2.1203e-01, -9.6432e-01, -9.4622e-01,  5.7054e-01,\n",
      "          9.4001e-01,  1.8941e-01,  9.9923e-01, -1.5256e-01,  9.1510e-01,\n",
      "          7.8499e-02, -5.6504e-01, -7.2291e-02, -4.5555e-01,  5.5808e-01,\n",
      "          1.8401e-01, -6.0199e-01,  2.3481e-01, -3.3689e-02, -2.7570e-01,\n",
      "         -5.3239e-01, -1.7641e-01, -4.0169e-01, -9.2079e-01, -3.1998e-01,\n",
      "          9.3809e-01, -7.1153e-02, -6.9783e-01,  6.7253e-02, -2.0274e-01,\n",
      "         -3.7808e-01,  8.4576e-01,  5.0007e-01,  3.6423e-01, -4.1740e-01,\n",
      "          3.6109e-01,  1.4876e-01,  3.5860e-01, -8.4838e-01,  2.6076e-01,\n",
      "          4.1683e-01, -3.1267e-01, -5.2371e-01, -9.6102e-01, -2.9391e-01,\n",
      "          5.4447e-01,  9.8303e-01,  7.4894e-01,  2.8395e-01,  1.4951e-01,\n",
      "         -2.6745e-01,  1.8009e-01, -9.3940e-01,  9.6656e-01, -1.0958e-01,\n",
      "          1.8235e-01,  1.6497e-01,  2.4865e-01, -8.2815e-01,  7.0525e-04,\n",
      "          8.1065e-01, -1.2265e-02, -8.1630e-01, -4.3830e-02, -4.7633e-01,\n",
      "         -3.8496e-01, -3.8118e-01,  4.5080e-01, -1.9782e-01, -3.4960e-01,\n",
      "          1.1252e-01,  8.9026e-01,  9.7578e-01,  6.9775e-01, -1.2651e-01,\n",
      "          5.7326e-01, -8.3355e-01, -4.0410e-01,  5.5685e-02,  1.7855e-01,\n",
      "          8.9321e-02,  9.8696e-01, -3.9308e-01, -7.7631e-02, -9.1122e-01,\n",
      "         -9.7244e-01,  5.5040e-03, -8.4574e-01, -2.4718e-02, -6.3462e-01,\n",
      "          4.9246e-01,  5.5000e-01,  1.1419e-01,  3.0862e-01, -9.6484e-01,\n",
      "         -7.7138e-01,  3.5477e-01, -2.1973e-01,  4.2417e-01, -2.4453e-01,\n",
      "          6.6965e-01,  6.5434e-01, -6.3527e-01,  8.0241e-01,  9.1417e-01,\n",
      "         -4.4787e-01, -6.7925e-01,  8.3256e-01, -2.6945e-01,  8.7207e-01,\n",
      "         -5.2282e-01,  9.8420e-01,  4.2696e-01,  4.8134e-01, -8.7702e-01,\n",
      "         -5.7352e-01, -8.7672e-01, -3.6707e-01,  7.0802e-02, -1.4201e-01,\n",
      "          6.3290e-01,  6.3346e-01,  2.9797e-01,  4.1055e-01, -6.0411e-01,\n",
      "          9.9386e-01, -7.6779e-01, -9.4760e-01,  2.8060e-01,  1.7029e-02,\n",
      "         -9.8417e-01,  3.4940e-01,  2.5224e-01, -4.0932e-01, -3.8472e-01,\n",
      "         -6.1061e-01, -9.4425e-01,  8.4996e-01,  1.8030e-01,  9.8473e-01,\n",
      "         -8.3625e-02, -8.9589e-01, -4.1394e-01, -8.9448e-01, -2.8790e-01,\n",
      "         -1.7622e-01,  1.2934e-01, -1.4377e-01, -9.4701e-01,  4.8915e-01,\n",
      "          5.5083e-01,  3.5019e-01, -5.7310e-01,  9.9580e-01,  9.9998e-01,\n",
      "          9.5247e-01,  8.9147e-01,  8.6635e-01, -9.9394e-01, -3.3778e-01,\n",
      "          9.9998e-01, -9.1078e-01, -1.0000e+00, -9.3294e-01, -5.2658e-01,\n",
      "          3.7828e-01, -1.0000e+00, -3.1776e-01,  7.3381e-02, -8.6886e-01,\n",
      "          3.6298e-01,  9.6532e-01,  9.8247e-01, -1.0000e+00,  8.0954e-01,\n",
      "          9.0122e-01, -6.6414e-01,  6.7350e-01, -3.1281e-01,  9.6308e-01,\n",
      "          5.0662e-01,  4.2693e-01, -2.2366e-01,  3.7596e-01, -8.1644e-01,\n",
      "         -8.3308e-01, -1.3696e-01, -4.6714e-01,  9.4581e-01,  1.8325e-01,\n",
      "         -7.8852e-01, -8.7354e-01, -5.8153e-02, -6.9013e-02, -3.4173e-01,\n",
      "         -9.4760e-01, -1.7820e-01, -1.3928e-01,  6.8355e-01,  4.5169e-02,\n",
      "          2.6814e-01, -6.8769e-01,  2.1210e-01, -2.3396e-01,  3.4735e-01,\n",
      "          6.8611e-01, -9.2963e-01, -5.2597e-01,  3.0077e-01, -3.7891e-01,\n",
      "         -4.7398e-01, -9.5464e-01,  9.4878e-01, -3.2371e-01,  3.0548e-01,\n",
      "          1.0000e+00, -1.6540e-01, -7.9525e-01,  5.4072e-01,  1.6751e-01,\n",
      "         -1.2475e-01,  1.0000e+00,  6.4986e-01, -9.5677e-01, -6.0413e-01,\n",
      "          3.7020e-01, -4.8581e-01, -4.7777e-01,  9.9866e-01, -2.5729e-01,\n",
      "         -2.4092e-01,  1.1121e-01,  9.6287e-01, -9.8189e-01,  9.3767e-01,\n",
      "         -8.7587e-01, -9.4305e-01,  9.3710e-01,  9.0091e-01, -3.8187e-01,\n",
      "         -6.1976e-01,  1.4977e-01, -5.8692e-01,  2.6118e-01, -9.5406e-01,\n",
      "          6.2279e-01,  4.8689e-01, -8.8540e-02,  8.5752e-01, -8.3492e-01,\n",
      "         -5.3720e-01,  2.5680e-01, -4.1266e-01,  2.2846e-01,  5.8122e-01,\n",
      "          4.3429e-01, -2.4578e-01,  1.2280e-01, -2.9919e-01, -3.1362e-01,\n",
      "         -9.6913e-01,  6.2339e-02,  1.0000e+00,  1.3627e-02, -1.2817e-01,\n",
      "         -3.8745e-01,  1.8794e-02, -1.8447e-01,  4.2568e-01,  5.1443e-01,\n",
      "         -2.2532e-01, -8.0441e-01,  1.4079e-01, -9.4743e-01, -9.8253e-01,\n",
      "          7.6432e-01,  1.4451e-01, -3.2291e-01,  9.9988e-01,  3.1823e-01,\n",
      "          1.5380e-01,  1.3672e-01,  7.6152e-01,  1.0233e-01,  5.8834e-01,\n",
      "          5.5959e-01,  9.6941e-01, -2.7468e-01,  6.1814e-01,  8.2141e-01,\n",
      "         -5.6555e-01, -2.7680e-01, -6.2172e-01, -8.9432e-02, -9.0074e-01,\n",
      "          1.2009e-01, -9.2751e-01,  9.4329e-01,  4.9252e-01,  2.7484e-01,\n",
      "          2.1843e-01,  6.6066e-02,  1.0000e+00, -2.2593e-01,  6.4518e-01,\n",
      "         -5.1134e-01,  8.2547e-01, -9.8957e-01, -7.8994e-01, -3.4983e-01,\n",
      "         -8.3618e-02, -3.6136e-01, -2.2800e-01,  2.0661e-01, -9.5846e-01,\n",
      "          3.5442e-01,  9.7110e-02, -9.7418e-01, -9.8264e-01,  2.9154e-01,\n",
      "          8.3737e-01,  6.6634e-02, -8.0501e-01, -6.1920e-01, -5.9091e-01,\n",
      "          1.7083e-01, -1.7838e-01, -9.0619e-01,  2.4375e-01, -1.7610e-01,\n",
      "          4.1765e-01, -2.2214e-01,  5.1794e-01,  5.8822e-01,  6.4995e-01,\n",
      "          1.3124e-01, -9.7190e-02, -8.3229e-02, -8.4713e-01,  8.4501e-01,\n",
      "         -7.9905e-01, -4.0447e-01, -2.0988e-01,  1.0000e+00, -5.1438e-01,\n",
      "          6.3233e-01,  7.9540e-01,  6.0073e-01, -1.3826e-01,  2.1753e-01,\n",
      "          6.5091e-01,  1.4618e-01, -6.2026e-01, -4.0101e-01, -7.0176e-01,\n",
      "         -3.3691e-01,  6.5601e-01, -8.1212e-02,  4.6828e-01,  6.5125e-01,\n",
      "          4.2083e-01,  1.4356e-01, -3.9187e-02, -4.4730e-02,  9.9797e-01,\n",
      "         -1.2616e-01,  5.6322e-02, -5.0923e-01,  7.9200e-03, -3.3530e-01,\n",
      "         -4.9022e-01,  1.0000e+00,  2.7262e-01, -1.3420e-01, -9.8275e-01,\n",
      "         -3.7159e-01, -8.9745e-01,  9.9994e-01,  8.0115e-01, -7.2103e-01,\n",
      "          6.0988e-01,  2.2475e-01, -1.7136e-01,  7.3667e-01, -1.3005e-01,\n",
      "         -2.5390e-01,  2.7347e-01,  9.6592e-02,  9.2964e-01, -4.7674e-01,\n",
      "         -9.5305e-01, -6.8485e-01,  3.5561e-01, -9.3918e-01,  9.9616e-01,\n",
      "         -4.4200e-01, -1.8582e-01, -4.1399e-01,  4.5030e-01,  6.3905e-01,\n",
      "         -1.7178e-01, -9.7449e-01, -5.1296e-02,  1.6738e-01,  9.3006e-01,\n",
      "          1.3585e-01, -5.9081e-01, -9.1515e-01,  1.1425e-01,  4.2950e-01,\n",
      "         -5.2346e-01, -8.7475e-01,  9.4331e-01, -9.7478e-01,  3.9603e-01,\n",
      "          1.0000e+00,  3.9404e-01, -6.0146e-01,  2.9374e-02, -5.1601e-01,\n",
      "          2.4861e-01,  2.4923e-01,  6.7270e-01, -9.2070e-01, -2.8287e-01,\n",
      "         -7.6749e-02,  1.7067e-01, -8.0916e-02,  1.2053e-01,  5.9641e-01,\n",
      "          1.7944e-01, -5.4069e-01, -5.6923e-01, -2.5089e-02,  4.6722e-01,\n",
      "          8.3220e-01, -3.5286e-01, -1.6219e-01,  9.4051e-02, -9.4971e-02,\n",
      "         -8.7677e-01, -2.7396e-01, -2.1794e-01, -9.9938e-01,  7.5556e-01,\n",
      "         -1.0000e+00, -1.7111e-01, -1.8512e-01, -2.3656e-01,  7.9198e-01,\n",
      "          2.8102e-01,  2.7825e-01, -6.7673e-01, -5.1072e-01,  6.6396e-01,\n",
      "          6.7959e-01, -1.5749e-01,  1.9009e-01, -6.7916e-01,  1.1148e-01,\n",
      "         -4.1110e-03, -7.4851e-03, -1.5137e-01,  8.2224e-01, -1.6503e-01,\n",
      "          1.0000e+00,  1.4966e-01, -6.3775e-01, -9.6420e-01,  2.2951e-01,\n",
      "         -2.5219e-01,  1.0000e+00, -9.1262e-01, -9.3021e-01,  2.5157e-01,\n",
      "         -6.5498e-01, -8.3972e-01,  2.0794e-01,  1.4597e-01, -5.7222e-01,\n",
      "         -5.5899e-01,  9.3297e-01,  9.1276e-01, -6.3061e-01,  3.3351e-01,\n",
      "         -3.3155e-01, -3.4310e-01,  5.5119e-02,  1.7480e-01,  9.7497e-01,\n",
      "          1.8068e-01,  8.6350e-01,  4.1295e-01,  6.8414e-02,  9.4591e-01,\n",
      "          2.3672e-01,  4.7458e-01,  5.9380e-02,  1.0000e+00,  2.6528e-01,\n",
      "         -8.8648e-01,  3.4260e-01, -9.7573e-01, -1.5098e-01, -9.4319e-01,\n",
      "          2.2633e-01,  2.1388e-01,  8.7123e-01, -9.6188e-02,  9.3925e-01,\n",
      "          2.4524e-02, -9.8376e-02, -1.0091e-01,  1.4681e-01,  3.8430e-01,\n",
      "         -8.7014e-01, -9.7812e-01, -9.7528e-01,  3.6960e-01, -3.8164e-01,\n",
      "         -4.7706e-02,  1.4460e-01,  1.0588e-01,  3.0843e-01,  4.1187e-01,\n",
      "         -1.0000e+00,  9.1188e-01,  3.6029e-01,  7.0125e-01,  9.3118e-01,\n",
      "          5.1771e-01,  2.7587e-01,  2.8873e-01, -9.7848e-01, -9.4348e-01,\n",
      "         -3.1992e-01, -2.0700e-01,  6.7060e-01,  6.4204e-01,  8.7050e-01,\n",
      "          3.0572e-01, -4.9178e-01, -2.6046e-01,  1.0768e-01, -5.5766e-01,\n",
      "         -9.8769e-01,  3.4500e-01, -1.2493e-01, -9.4611e-01,  9.4435e-01,\n",
      "         -2.2333e-01, -1.4135e-01,  1.9776e-01, -3.9046e-01,  9.3880e-01,\n",
      "          6.8928e-01,  3.8767e-01,  1.1025e-01,  5.1922e-01,  8.3273e-01,\n",
      "          9.4730e-01,  9.7341e-01, -5.7427e-01,  7.8560e-01, -6.8622e-02,\n",
      "          4.1192e-01,  6.9466e-01, -9.3774e-01,  2.3971e-01,  1.3474e-01,\n",
      "         -4.2771e-01,  1.6046e-01, -2.4939e-01, -9.7068e-01,  5.0688e-01,\n",
      "         -2.9860e-01,  5.1771e-01, -3.1591e-01,  1.0868e-01, -3.8842e-01,\n",
      "         -1.5632e-01, -6.4045e-01, -5.4585e-01,  6.6964e-01,  3.3962e-01,\n",
      "          8.5658e-01,  5.4788e-01, -1.0526e-01, -5.1888e-01, -1.6210e-01,\n",
      "         -5.4360e-01, -8.8314e-01,  8.7812e-01,  4.3410e-02, -2.1279e-01,\n",
      "          3.2289e-01, -1.8518e-01,  6.5461e-01, -6.4462e-02, -3.7191e-01,\n",
      "         -3.7232e-01, -7.5502e-01,  8.0540e-01, -1.7700e-01, -5.2501e-01,\n",
      "         -5.6918e-01,  3.7711e-01,  3.8391e-01,  9.9864e-01, -5.3484e-01,\n",
      "         -5.4315e-01, -7.2384e-02, -2.4831e-01,  4.2074e-01, -2.2462e-01,\n",
      "         -1.0000e+00,  2.4747e-01,  7.5571e-02,  3.7574e-01, -7.1564e-02,\n",
      "          3.3389e-01, -1.6045e-01, -9.6561e-01, -1.6314e-01,  7.3814e-02,\n",
      "          3.5186e-01, -4.4838e-01, -2.2300e-01,  5.7876e-01,  6.1218e-01,\n",
      "          7.0053e-01,  8.2005e-01,  1.1350e-01,  2.0501e-01,  6.4786e-01,\n",
      "         -2.2238e-01, -6.3388e-01,  8.9791e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "# BERT와 GPT-2모델을 활용할 때 허깅페이스 트랜스포머 코드 비교\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "text = \"What is Huggingface Transformers?\"\n",
    "# BERT 모델 활용\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\") # 모델 불러오기\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') # 토크나이저 불러오기\n",
    "encoded_input = bert_tokenizer(text, return_tensors='pt') # 입력 토큰화\n",
    "bert_output = bert_model(**encoded_input) # 모델에 입력\n",
    "print(bert_output)\n",
    "# # GPT-2 모델 활용\n",
    "# gpt_model = AutoModel.from_pretrained('gpt2') # 모델 불러오기\n",
    "# gtp_tokenizer = AutoTokenizer.from_pretrained('gpt2') # 토크나이저 불러오기\n",
    "# encoded_input = gpt_tokenizer(text, return_tensors='pt') # 입력 토큰화\n",
    "# gpt_output = gpt_model(**encoded_input) # 모델에 입력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 허깅페이스 허브 탐색하기\n",
    "### 3.2.1 모델 허브\n",
    "어떤 작업(Tasks)에 사용하는지, 어떤 언어(Languages)로 학습된 모델인지 등 자양한 기준으로 모델이 분류됨 <br>\n",
    "모델 허브에서는 자연어 처리(NLP)뿐만 아니라 컴퓨터 비전, 오디오 처리, 멀티 모달 등 다양한 작업 분야의 모델을 제공 <br>\n",
    "### 3.2.2 데이터셋 허브\n",
    "분류 기준에 데이터셋 크기, 데이터 유형 등이 추가로 있고 선택한 기준에 맞는 데이터셋을 보여줌 <br>\n",
    "### 3.2.3 모델 데모를 공개하고 사용할 수 있는 스페이스\n",
    "사용자가 자신의 모델 데모를 간편하게 공개 <br>\n",
    "스페이스를 활용하면 별도의 복잡한 웹 페이지 개발 없이 모델 데모를 공유할 수 있음 <br>\n",
    "## 3.3 허깅페이스 라이브러리 사용법 익히기\n",
    "모델을 학습시키거나 추론하기 위해서는 모델, 토크나이저, 데이터셋이 필요\n",
    "### 3.3.1 모델 활용하기\n",
    "허깅페이스에서는 모델을 바디(body)와 헤드(head)로 구분함.  같은 바디를 사용하면서 다른 작업에 사용할 수 있도록 만들기 위해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(32000, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "model_id = 'klue/roberta-base'\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AutoModel: 모델의 바디를 불러오는 클래스\n",
    "    - from_pretrained()메서드에서 인자로 받는 model_id 에 맞춰 적절한 클래스를 가져옴\n",
    "- model_id\n",
    "    - 허깅페이스 모델 허브의 저장소 경로(klue/roberta-base)인 경우 모델 허브에서 모델을 다운로드\n",
    "    - 로컬인 경우, 지정한 로컬 경로에서 모델을 불러옴\n",
    "<br>\n",
    "\n",
    "_AutoModel은 어떻게 klue/roberta-base 저장소의 모델이 RoBERTa계열의 모델인지 알 수 있을까?_\n",
    "- 허깅페이스 모델을 저장할 때 config.json이 함께 저장됨\n",
    "- 해당 설정 파일에 모델의 종류(model_type)\n",
    "- 여러 설정 파라미터(num_attention_heads, num_hidden_layers 등), 어휘 사전 크기(vocab_size), 토크나이저 클래스(tokenzier_class)등이 저장됨\n",
    "- AutoModel 과 AutoTokenizer 클래스는 config.json 파일을 참고해 적절한 모델과 토크나이저를 불러옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=28, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 분류 헤드가 포함된 모델 불러오기\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model_id = 'SamLowe/roberta-base-go_emotions'\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "classification_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 토크나이저 활용하기\n",
    "토크나이저는 텍스트를 토큰 단위로 나누고 각 토큰을 대응하는 토큰 아이디로 변환함 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 9157, 7461, 2190, 2259, 8509, 2138, 1793, 2855, 5385, 2200, 20950, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', '토크', '##나이', '##저', '##는', '텍스트', '##를', '토', '##큰', '단위', '##로', '나눈다', '[SEP]']\n",
      "[CLS] 토크나이저는 텍스트를 토큰 단위로 나눈다 [SEP]\n",
      "토크나이저는 텍스트를 토큰 단위로 나눈다\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 불러오기\n",
    "from transformers import AutoTokenizer\n",
    "model_id = 'klue/roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# 토크나이저 사용하기\n",
    "tokenized = tokenizer(\"토크나이저는 텍스트를 토큰 단위로 나눈다\")\n",
    "# 토큰 id의 리스트인 input_ids\n",
    "# 토큰이 실제 텍스트인지 아니면 길이를 맞추기 위해 추가한 패딩인지 알려주는 attention_mask: 1이면 실제 토큰, 0이면 패딩\n",
    "# 토큰이 속한 문장의 아이디를 알려주는 token_type_ids: 0 이면 일반적으로 첫번째 문장\n",
    "print(tokenized)\n",
    "print(tokenizer.convert_ids_to_tokens(tokenized['input_ids']))\n",
    "print(tokenizer.decode(tokenized['input_ids'])) # id를 다시 텍스트로 돌리고 싶을 때\n",
    "print(tokenizer.decode(tokenized['input_ids'], skip_special_tokens=True)) # 특수 토큰을 제외하고 텍스트로 돌리고 싶을 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2], [0, 864, 1141, 3135, 6265, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 토크나이저에 여러 문장 넣기\n",
    "tokenizer(['첫 번째 문장', '두 번째 문장'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 하나의 데이터에 여러 문장이 들어가는 경우\n",
    "tokenizer([['첫 번째 문장', '두 번째 문장']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS] 첫 번째 문장 [SEP]', '[CLS] 두 번째 문장 [SEP]']\n",
      "['[CLS] 첫 번째 문장 [SEP] 두 번째 문장 [SEP]']\n"
     ]
    }
   ],
   "source": [
    "# 토큰 아이디를 문자열로 복원\n",
    "first_tokenized_result = tokenizer(['첫 번째 문장', '두 번째 문장'])['input_ids']\n",
    "print(tokenizer.batch_decode(first_tokenized_result))\n",
    "\n",
    "second_tokenized_result = tokenizer([['첫 번째 문장', '두 번째 문장']])['input_ids']\n",
    "print(tokenizer.batch_decode(second_tokenized_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT는 학습할 때 2개의 문장이 서로 이어지는지 맞추는 NSP(Next Sentence Prediction)작업을 활용 <br>\n",
    "이를 위해 문장을 구분하는 토큰 타입 아이디를 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[2, 1656, 1141, 3135, 6265, 3, 864, 1141, 3135, 6265, 3]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2, 864, 1141, 3135, 6265, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "{'input_ids': [[0, 9502, 3645, 2, 2, 10815, 3645, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "# BERT 토크나이저와 RoBERTa 토크나이저\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "print(bert_tokenizer([['첫 번째 문장', '두 번째 문장']])) # 문장에 따라 토큰 타입 아이디를 구분\n",
    "\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained('klue/roberta-base')\n",
    "print(roberta_tokenizer([['첫 번째 문장', '두 번째 문장']])) # NSP작업을 학습 과정에서 제거했기 때문에 문장 토큰 구분이 필요 없음\n",
    "\n",
    "en_roberta_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "print(en_roberta_tokenizer([['first sentence', 'second sentence']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attention_mask는 해당 토큰이 패딩 토큰인지 실제 데이터인지에 대한 정보를 담고 있음 <br>\n",
    "패딩은 모델에 입력하는 토큰 아이디의 길이를 맞추기 위해 추가하는 특수 토큰 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 1656, 1141, 3135, 6265, 2073, 1599, 2062, 18, 2, 1, 1, 1, 1, 1, 1, 1], [0, 864, 1141, 3135, 6265, 2073, 1656, 1141, 3135, 6265, 2178, 2062, 831, 647, 2062, 18, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(['첫 번째 문장은 짧다.', '두 번째 문장은 첫 번째 문장보다 더 길다.'], padding='longest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 데이터셋 활용하기\n",
    "datasets 라이브러리를 사용하면 허깅페이스 허브에서 살펴봤던 데이터셋을 코들 불러올 수 있음 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers'],\n",
       "        num_rows: 17554\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['title', 'context', 'news_category', 'source', 'guid', 'is_impossible', 'question_type', 'question', 'answers'],\n",
       "        num_rows: 5841\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KLUE MRC 데이터셋 다운로드\n",
    "from datasets import load_dataset\n",
    "klue_mrc_dataset = load_dataset('klue', 'mrc')\n",
    "# klue_mrc_dataset_only_train = load_dataset('klue', 'mrc', split='train') # 유형이 train인 데이터셋만 불러오기\n",
    "klue_mrc_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 로컬의 데이터 활용하기\n",
    "# from datasets import load_dataset\n",
    "# # 로컬의 csv 데이터 파일을 활용\n",
    "# dataset = load_dataset(\"csv\", data_files=\"my_file.csv\")\n",
    "\n",
    "# # 파이썬 딕셔너리 활용\n",
    "# from datasets import Dataset\n",
    "# my_dict = {\"a\": [1, 2, 3]}\n",
    "# dataset = Dataset.from_dict(my_dict)\n",
    "\n",
    "# # 판다스 데이터프레임 활용\n",
    "# from datasets import Dataset\n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame({\"a\": [1, 2, 3]})\n",
    "# dataset = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 모델 학습시키기\n",
    "허깅페이스 트랜스포머에서는 간편하게 모델학습을 수행할 수 있도록 학습 과정을 추상화한 트레이너(Trainer) API를 제공함 <br>\n",
    "학습을 간편하게 살 수 있다는 장점이 있지만 내부에서 어떤 과정을 거치는지 알기 어려움 <br>\n",
    "### 3.4.1 데이터 준비\n",
    "실습데이터: KLUE 데이터셋의 YNAT 서브셋 활용 (연합 뉴스 기사의 제목과 기사가 속한 카테고리 정보가 있음) <br>\n",
    "연합 뉴스 기사의 제목을 바탕으로 카테고리를 예측하는 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['guid', 'title', 'label', 'url', 'date'],\n",
       "    num_rows: 45678\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습에 사용할 연합 뉴스 데이터셋 다운로드\n",
    "klue_tc_train = load_dataset('klue', 'ynat', split='train')\n",
    "klue_tc_eval = load_dataset('klue', 'ynat', split='validation')\n",
    "klue_tc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': 'ynat-v1_train_00000',\n",
       " 'title': '유튜브 내달 2일까지 크리에이터 지원 공간 운영',\n",
       " 'label': 3,\n",
       " 'url': 'https://news.naver.com/main/read.nhn?mode=LS2D&mid=shm&sid1=105&sid2=227&oid=001&aid=0008508947',\n",
       " 'date': '2016.06.30. 오전 10:36'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 개별 데이터 형태 확인\n",
    "klue_tc_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IT과학', '경제', '사회', '생활문화', '세계', '스포츠', '정치']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# features속성은 데이터셋의 정보를 저장하고 있음\n",
    "klue_tc_train.features['label'].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['title', 'label'],\n",
       "    num_rows: 45678\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 분류모델 학습 시 불필요한 guid, url, date컬럼 제거\n",
    "klue_tc_train = klue_tc_train.remove_columns(['guid', 'url', 'date'])\n",
    "klue_tc_eval = klue_tc_eval.remove_columns(['guid', 'url', 'date'])\n",
    "klue_tc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['IT과학', '경제', '사회', '생활문화', '세계', '스포츠', '정치'], id=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 카테고리를 확인하기 쉽도록 label_str컬럼 추가 \n",
    "klue_tc_train.features['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'경제'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# int2str() ID를 카테고리로 변환\n",
    "klue_tc_train.features['label'].int2str(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '유튜브 내달 2일까지 크리에이터 지원 공간 운영', 'label': 3, 'label_str': '생활문화'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klue_tc_label = klue_tc_train.features['label']\n",
    "def make_str_label(batch):\n",
    "    batch['label_str'] = klue_tc_label.int2str(batch['label'])\n",
    "    return batch\n",
    "\n",
    "klue_tc_train = klue_tc_train.map(make_str_label, batched=True, batch_size=1000)\n",
    "klue_tc_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습/검증/테스트 데티터셋 분할\n",
    "train_dataset = klue_tc_train.train_test_split(test_size=10000, shuffle=True, seed=42)['test']\n",
    "dataset = klue_tc_eval.train_test_split(test_size=1000, shuffle=True, seed=42)\n",
    "test_dataset = dataset['test']\n",
    "valid_dataset = dataset['train'].train_test_split(test_size=1000, shuffle=True, seed=42)['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 트레이너 API를 사용해 학습하기\n",
    "학습에 사용할 분류 모델을 불러오기 위해 AutoModelForSequenceClassification 클래스로 klue/roberta-base 모델을 불러옴 <br>\n",
    "앞서 살펴본 대로 모델 바디의 파라미터만 있는 klue/roberta-base 모델을 불러오면 분류 헤드 부분은 랜덤으로 초기화됨 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9501a61a9bcc42f988fcca74aafcb81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Trainer 를 사용한 학습: (1) 준비\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer\n",
    ")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"title\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "model_id = 'klue/roberta-base'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=len(train_dataset.features['label'].names))\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "valid_dataset = valid_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TrainingArguments 에 학습인자를 입력 <br>\n",
    "- num_train_epochs: 학습 에포크 수 1\n",
    "- per_device_train_batch_size: 배치 크기 8\n",
    "- output_dir: 결과를 저장할 폴더 results\n",
    "- evaluation_strategy: 평가를 수행할 빈도 epoch\n",
    "- compute_metrics: 학습이 잘 이뤄지고 있는지 확인할 때 사용할 평가 지표\n",
    "    - 모델의 예측결과인 eval_pred를 입력으로 받아 예측 결과 중 가장 큰 값을 갖는 클래스를 np.argmax함수로 뽑아 predictions 변수에 저장\n",
    "    - predictions와 정답이 저장된 labels가 같은 값을 갖는 결과의 비율을 정확도로 결과 딕셔너리에 저장해 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer를 사용한 학습: (2) 학습 인자와 평가 함수 정의\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qd/lq70ttjn6cx056nn_29xpcd00000gn/T/ipykernel_67578/883557562.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 23:43, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.529500</td>\n",
       "      <td>0.514199</td>\n",
       "      <td>0.835000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 00:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.46936774253845215,\n",
       " 'eval_accuracy': 0.846,\n",
       " 'eval_runtime': 37.2772,\n",
       " 'eval_samples_per_second': 26.826,\n",
       " 'eval_steps_per_second': 3.353,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainer를 사용한 학습: (3) 학습 진행\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 트레이너 API를 사용하지 않고 학습하기\n",
    "Trainer 를 사용한 예제와 비슷하게 모델과 토크나이저를 불러오고, 토큰화에 사용할 tokenize_function함수를 정의 <br>\n",
    "Trainer를 사용하는 예제에서는 Trainer가 내부적으로 수행하던 GPU로의 모델 이동(model.to(device))을 직접 수행해야함 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trainer를 사용하지 않는 학습: (1) 학습을 위한 모델과 토크나이저 준비\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "def tokenize_function(examples): # 제목(title) 컬럼에 대한 토큰화\n",
    "    return tokenizer(examples[\"title\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# 모델과 토크나이저 불러오기\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_id = \"klue/roberta-base\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=len(train_dataset.features['label'].names))\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer를 사용하지 않는 학습: (2) 학습을 위한 데이터 준비\n",
    "def make_dataloader(dataset, batch_size, shuffle=True):\n",
    "    dataset = dataset.map(tokenize_function, batched=True).with_format(\"torch\")\n",
    "    # 데이터셋에 토큰화 수행\n",
    "    dataset = dataset.rename_column(\"label\", \"labels\") #컬럼 이름 변경\n",
    "    dataset = dataset.remove_columns(column_names=[\"title\"])\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "# 데이터로더 만들기\n",
    "train_dataloader = make_dataloader(train_dataset, batch_size=8, shuffle=True)\n",
    "valid_dataloader = make_dataloader(valid_dataset, batch_size=8, shuffle=False)\n",
    "test_dataloader = make_dataloader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer를 사용하지 않는 학습: (3) 학습을 위한 함수 정의\n",
    "def train_epoch(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader):\n",
    "        optimizer.zero_grad() # 그래디언트 초기화\n",
    "        input_ids = batch['input_ids'].to(device) # 이것도 왜 따로 device에 넣지? 모델에 입력할 토큰 아이디\n",
    "        attention_mask = batch['attention_mask'].to(device) # 왜 이것을 device로 넣지? 모델에 입력할 어텐션 마스크\n",
    "        labels = batch['lables'].to(device) # 모델에 입력할 정답 레이블\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels) # 모델 계산\n",
    "        loss = outputs.loss # 손실\n",
    "        loss.backward() # 역전파 왜하지?\n",
    "        optimizer.step() # 모델 업데이트\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 를 사용하지 않는 학습: (4) 평가를 위한 함수 정의\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad(): # 그래디언트 계산 안함\n",
    "        for batch in tqdm(data_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            logits = outputs.logits\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        accuracy = np.mean(np.asarray(predictions) == np.asarray(true_labels))\n",
    "        return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer를 사용하지 않는 학습: (5) 학습 수행\n",
    "num_epochs = 1\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1} / {num_epochs}\")\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer)\n",
    "    print(f\"Training loss: {train_loss}\")\n",
    "    valid_loss, valid_accuracy = evaluate(model, valid_dataloader)\n",
    "    print(f\"Validation loss: {valid_loss}\")\n",
    "    print(f\"Validation accuracy: {valid_accuracy}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
