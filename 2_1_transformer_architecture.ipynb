{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_text_list:  ['나는', '최근', '파리', '여행을', '다녀왔다']\n",
      "str2idx:  {'나는': 0, '최근': 1, '파리': 2, '여행을': 3, '다녀왔다': 4}\n",
      "idx2str:  {0: '나는', 1: '최근', 2: '파리', 3: '여행을', 4: '다녀왔다'}\n",
      "input_ids:  [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"나는 최근 파리 여행을 다녀왔다\"\n",
    "input_text_list = input_text.split()\n",
    "print(\"input_text_list: \", input_text_list)\n",
    "\n",
    "str2idx = {word:idx for idx, word in enumerate(input_text_list)}\n",
    "idx2str = {idx:word for idx, word in enumerate(input_text_list)}\n",
    "print(\"str2idx: \", str2idx)\n",
    "print(\"idx2str: \", idx2str)\n",
    "\n",
    "# token to token id\n",
    "input_ids = [str2idx[word] for word in input_text_list]\n",
    "print(\"input_ids: \", input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 토큰 임베딩으로 변환하기\n",
    "\n",
    "딥러닝 모델이 텍스트 데이터를 처리하기 위해 입력으로 들어오는 토큰과 토큰 사이의 관계를 계산해야함.<br>\n",
    "즉, 토큰의 의미를 숫자로 표현해야함 <br>\n",
    "ID는 하나의 숫자일 뿐 토큰의 의미를 담을 수 없음 <br>\n",
    "최소 2개 이상의 숫자 집한인 vector로 표현되어야함 <br>\n",
    "=> 이것이 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 16])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "embedding_dim = 16 # 토큰하나를 16차원으로 변환\n",
    "# embedding layer 만들기\n",
    "embed_layer = nn.Embedding(len(str2idx), embedding_dim) # len(str2idx) 사전의 크기\n",
    "input_embeddings = embed_layer(torch.tensor(input_ids)) # 토큰을 임베딩 층을 통해 임베딩으로 변환\n",
    "input_embeddings = input_embeddings.unsqueeze(0) # 차원 확장\n",
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- embedding layer는 토큰의 의미를 담아 벡터로 변환하는 단계가 아님\n",
    "- embedding layer가 단어의 의미를 담기 위해서 딥러닝 모델이 학습 데이터로 훈련되어야 함\n",
    "- 여기서 ML과 DL의 차이점\n",
    "    - DL은 모델이 학습하는 과정에서 데이터의 의미를 잘 담은 embedding을 만드는 방법도 함께 학습함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 위치 인코딩\n",
    "#### RNN과 Transformer 의 차이\n",
    "RNN은 입력을 순차적으로 처리하기 떄문에 순서 정보가 고려 <br>\n",
    "Transformer는 모든 입력을 동시에 처리하는 과정에서 순서 정보가 사라짐 <br>\n",
    "Text 에서는 순차 정보가 중요한데 Transformer를 사용하게 되면 이 역할을 인코딩이 담당 <br>\n",
    "\n",
    "- Attention is All you need 에서 sine, cosine을 활용한 수식으로 위치에 대하 정보 입력\n",
    "- 절대적 위치 인코딩 (Absolute position encoding)\n",
    "    - 위치 인코딩도 위치에 따른 임베딩 층을 추가해 학습 데이터를 통해 학습 -> 모델로 추론을 수행\n",
    "    - 수식을 통해 위치 정보를 추가하는 방식\n",
    "    - 임베딩으로 위치 정보를 학습 하는 방식\n",
    "    - 간단하게 구현 가능\n",
    "    - 토큰과 토큰 사이의 상대적인 위치 정보는 활용하지 않음\n",
    "    - 긴 텍스트를 추론하는 경우 성능이 떨어짐\n",
    "- 상대적 위치 인코딩 (Relative position encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위치 정보를 학습하는 방식의 절대적 위치 인코딩\n",
    "embedding_dim = 16\n",
    "max_position = 12 # 최대 토큰 수\n",
    "embed_layer = nn.Embedding(len(str2idx), embedding_dim)\n",
    "position_embed_layer = nn.Embedding(max_position, embedding_dim) # 새로운 임베딩 층 추가\n",
    "# 위치 인덱스에 따라 임베딩을 더하도록 구현\n",
    "position_ids = torch.arange(len(input_ids), dtype=torch.long).unsqueeze(0)\n",
    "position_encodings = position_embed_layer(position_ids)\n",
    "token_embeddings = embed_layer(torch.tensor(input_ids))\n",
    "print(position_encodings)\n",
    "print(token_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (yy)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
